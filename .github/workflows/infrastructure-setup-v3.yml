name: Infrastructure Setup V3

on:
  workflow_dispatch:
    inputs:
      environment:
        description: 'Target Environment'
        required: true
        default: 'dev'
        type: choice
        options:
        - dev
        - stage
        - prod
      action:
        description: 'Action to perform'
        required: true
        default: 'create'
        type: choice
        options:
        - create
        - destroy
      vpc_id:
        description: 'VPC ID (optional - if not provided, will use default VPC or create new one for prod)'
        required: false
        type: string
      create_vpc:
        description: 'Create new VPC for production? (only used if vpc_id not provided and env is prod)'
        required: false
        default: 'true'
        type: choice
        options:
        - 'true'
        - 'false'
        - 'auto'  # Automatically find and reuse suitable VPC

env:
  AWS_REGION: ap-south-1
  PROJECT_NAME: sabpaisa-tokenization

jobs:
  setup-infrastructure:
    runs-on: ubuntu-latest
    environment: ${{ github.event.inputs.environment }}
    
    outputs:
      db_endpoint: ${{ steps.database.outputs.endpoint }}
      db_password: ${{ steps.database.outputs.password }}
      alb_dns: ${{ steps.alb.outputs.dns_name }}
      vpc_id: ${{ steps.vpc.outputs.vpc_id }}
    
    steps:
    - name: Checkout code
      uses: actions/checkout@v4
      
    - name: Configure AWS credentials
      uses: aws-actions/configure-aws-credentials@v4
      with:
        aws-access-key-id: ${{ secrets.AWS_ACCESS_KEY_ID }}
        aws-secret-access-key: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
        aws-region: ${{ env.AWS_REGION }}
    
    - name: Set environment variables
      run: |
        echo "ENV_NAME=${{ github.event.inputs.environment }}" >> $GITHUB_ENV
        echo "DB_IDENTIFIER=sabpaisa-tokenization-${{ github.event.inputs.environment }}" >> $GITHUB_ENV
        echo "ALB_NAME=sabpaisa-token-api-${{ github.event.inputs.environment }}-alb" >> $GITHUB_ENV
    
    - name: Setup VPC and Networking
      id: vpc
      if: github.event.inputs.action != 'destroy'
      run: |
        echo "ðŸŒ Setting up VPC and Networking"
        echo "================================="
        
        # Check if VPC ID was provided
        if [ -n "${{ github.event.inputs.vpc_id }}" ]; then
          echo "Using provided VPC ID: ${{ github.event.inputs.vpc_id }}"
          VPC_ID="${{ github.event.inputs.vpc_id }}"
        else
          # Try to find default VPC
          VPC_ID=$(aws ec2 describe-vpcs --filters "Name=isDefault,Values=true" --query 'Vpcs[0].VpcId' --output text 2>/dev/null || echo "None")
          
          if [ "$VPC_ID" = "None" ] || [ -z "$VPC_ID" ]; then
            # Handle auto mode first
            if [ "${{ github.event.inputs.environment }}" = "prod" ] && [ "${{ github.event.inputs.create_vpc }}" = "auto" ]; then
              echo "No default VPC found. Auto mode: Looking for a suitable existing VPC..."
              
              # List all VPCs first for debugging
              echo "Available VPCs:"
              aws ec2 describe-vpcs --query 'Vpcs[*].[VpcId,CidrBlock,State]' --output table
              
              # Find VPCs with at least 2 subnets
              SUITABLE_VPC=""
              for vpc in $(aws ec2 describe-vpcs --query 'Vpcs[?State==`available`].VpcId' --output text); do
                echo "Checking VPC: $vpc"
                SUBNET_COUNT=$(aws ec2 describe-subnets --filters "Name=vpc-id,Values=$vpc" --query 'length(Subnets)' --output text 2>/dev/null || echo "0")
                echo "  Subnet count: $SUBNET_COUNT"
                
                if [ "$SUBNET_COUNT" -ge 2 ]; then
                  SUITABLE_VPC="$vpc"
                  echo "  âœ“ VPC has sufficient subnets"
                  break
                fi
              done
              
              if [ -n "$SUITABLE_VPC" ] && [ "$SUITABLE_VPC" != "" ]; then
                echo "âœ… Found suitable VPC: ${SUITABLE_VPC}"
                VPC_ID="${SUITABLE_VPC}"
                
                # Show VPC details
                VPC_NAME=$(aws ec2 describe-vpcs --vpc-ids ${VPC_ID} --query 'Vpcs[0].Tags[?Key==`Name`].Value|[0]' --output text 2>/dev/null || echo "No Name")
                VPC_CIDR=$(aws ec2 describe-vpcs --vpc-ids ${VPC_ID} --query 'Vpcs[0].CidrBlock' --output text)
                echo "VPC Name: ${VPC_NAME}"
                echo "VPC CIDR: ${VPC_CIDR}"
                
                # Show subnet details
                echo "Subnets in this VPC:"
                aws ec2 describe-subnets --filters "Name=vpc-id,Values=${VPC_ID}" --query 'Subnets[*].[SubnetId,AvailabilityZone,CidrBlock]' --output table
                
                echo "Proceeding with existing VPC..."
              else
                echo "âŒ No suitable VPC found with at least 2 subnets"
                echo ""
                echo "VPC subnet summary:"
                for vpc in $(aws ec2 describe-vpcs --query 'Vpcs[*].VpcId' --output text); do
                  SUBNET_COUNT=$(aws ec2 describe-subnets --filters "Name=vpc-id,Values=$vpc" --query 'length(Subnets)' --output text 2>/dev/null || echo "0")
                  echo "  VPC $vpc has $SUBNET_COUNT subnet(s)"
                done
                echo ""
                echo "Please ensure at least one VPC has 2 or more subnets, or provide a specific VPC ID"
                exit 1
              fi
            elif [ "${{ github.event.inputs.environment }}" = "prod" ] && [ "${{ github.event.inputs.create_vpc }}" = "true" ]; then
              echo "No default VPC found. Checking VPC limits..."
              
              # Check current VPC count
              CURRENT_VPC_COUNT=$(aws ec2 describe-vpcs --query 'length(Vpcs)' --output text)
              echo "Current VPC count: ${CURRENT_VPC_COUNT}"
              
              # Check VPC limit (usually 5)
              VPC_LIMIT=$(aws service-quotas get-service-quota \
                --service-code vpc \
                --quota-code L-F678F1CE \
                --query 'Quota.Value' \
                --output text 2>/dev/null || echo "5")
              echo "VPC limit: ${VPC_LIMIT}"
              
              if [ "${CURRENT_VPC_COUNT}" -ge "${VPC_LIMIT}" ]; then
                echo "âŒ VPC limit reached (${CURRENT_VPC_COUNT}/${VPC_LIMIT})"
                echo ""
                echo "Existing VPCs:"
                aws ec2 describe-vpcs --query 'Vpcs[*].[VpcId,CidrBlock,Tags[?Key==`Name`].Value|[0]]' --output table
                echo ""
                echo "Options to proceed:"
                echo "1. Delete an unused VPC and re-run the pipeline"
                echo "2. Use an existing VPC by providing its ID in the 'vpc_id' parameter"
                echo "3. Request a VPC limit increase from AWS Support"
                echo ""
                echo "Looking for existing VPC tagged for this project..."
                EXISTING_PROJECT_VPC=$(aws ec2 describe-vpcs \
                  --filters "Name=tag:Name,Values=${PROJECT_NAME}-*" \
                  --query 'Vpcs[0].VpcId' \
                  --output text 2>/dev/null || echo "None")
                
                if [ "$EXISTING_PROJECT_VPC" != "None" ] && [ -n "$EXISTING_PROJECT_VPC" ]; then
                  echo "Found existing project VPC: ${EXISTING_PROJECT_VPC}"
                  echo "You can re-run this pipeline with vpc_id=${EXISTING_PROJECT_VPC}"
                fi
                
                # If create_vpc is 'auto', try to find a suitable VPC
                if [ "${{ github.event.inputs.create_vpc }}" = "auto" ]; then
                  echo ""
                  echo "Auto mode: Looking for a suitable VPC with at least 2 subnets..."
                  
                  # Find VPCs with at least 2 subnets
                  SUITABLE_VPC=$(aws ec2 describe-vpcs --query 'Vpcs[?State==`available`].VpcId' --output text | while read vpc; do
                    SUBNET_COUNT=$(aws ec2 describe-subnets --filters "Name=vpc-id,Values=$vpc" --query 'length(Subnets)' --output text)
                    if [ "$SUBNET_COUNT" -ge 2 ]; then
                      echo "$vpc"
                      break
                    fi
                  done)
                  
                  if [ -n "$SUITABLE_VPC" ] && [ "$SUITABLE_VPC" != "None" ]; then
                    echo "âœ… Found suitable VPC: ${SUITABLE_VPC}"
                    VPC_ID="${SUITABLE_VPC}"
                    echo "vpc_id=${VPC_ID}" >> $GITHUB_OUTPUT
                    echo "Proceeding with existing VPC..."
                  else
                    echo "âŒ No suitable VPC found with at least 2 subnets"
                    exit 1
                  fi
                else
                  exit 1
                fi
              fi
              
              # Only create VPC if we haven't found one and limit not reached
              if [ -z "$VPC_ID" ] || [ "$VPC_ID" = "None" ]; then
                echo "Creating new VPC for production..."
                
                # Create VPC
                VPC_ID=$(aws ec2 create-vpc \
                  --cidr-block 10.0.0.0/16 \
                  --tag-specifications "ResourceType=vpc,Tags=[{Key=Name,Value=${PROJECT_NAME}-${ENV_NAME}-vpc}]" \
                  --query 'Vpc.VpcId' \
                  --output text)
                
                # Enable DNS hostnames
                aws ec2 modify-vpc-attribute --vpc-id ${VPC_ID} --enable-dns-hostnames
                
                # Create Internet Gateway
                IGW_ID=$(aws ec2 create-internet-gateway \
                  --tag-specifications "ResourceType=internet-gateway,Tags=[{Key=Name,Value=${PROJECT_NAME}-${ENV_NAME}-igw}]" \
                  --query 'InternetGateway.InternetGatewayId' \
                  --output text)
                
                # Attach Internet Gateway to VPC
                aws ec2 attach-internet-gateway --vpc-id ${VPC_ID} --internet-gateway-id ${IGW_ID}
                
                # Create subnets in different AZs
                AVAILABILITY_ZONES=$(aws ec2 describe-availability-zones \
                  --query 'AvailabilityZones[?State==`available`].[ZoneName]' \
                  --output text | head -3)
                
                SUBNET_IDS=""
                SUBNET_INDEX=1
                
                for AZ in ${AVAILABILITY_ZONES}; do
                  SUBNET_ID=$(aws ec2 create-subnet \
                    --vpc-id ${VPC_ID} \
                    --cidr-block "10.0.${SUBNET_INDEX}.0/24" \
                    --availability-zone ${AZ} \
                    --tag-specifications "ResourceType=subnet,Tags=[{Key=Name,Value=${PROJECT_NAME}-${ENV_NAME}-subnet-${AZ}}]" \
                    --query 'Subnet.SubnetId' \
                    --output text)
                  
                  SUBNET_IDS="${SUBNET_IDS} ${SUBNET_ID}"
                  SUBNET_INDEX=$((SUBNET_INDEX + 1))
                done
                
                # Create route table
                ROUTE_TABLE_ID=$(aws ec2 create-route-table \
                  --vpc-id ${VPC_ID} \
                  --tag-specifications "ResourceType=route-table,Tags=[{Key=Name,Value=${PROJECT_NAME}-${ENV_NAME}-rt}]" \
                  --query 'RouteTable.RouteTableId' \
                  --output text)
                
                # Create route to Internet Gateway
                aws ec2 create-route \
                  --route-table-id ${ROUTE_TABLE_ID} \
                  --destination-cidr-block 0.0.0.0/0 \
                  --gateway-id ${IGW_ID}
                
                # Associate route table with subnets
                for SUBNET_ID in ${SUBNET_IDS}; do
                  aws ec2 associate-route-table \
                    --subnet-id ${SUBNET_ID} \
                    --route-table-id ${ROUTE_TABLE_ID}
                done
                
                echo "âœ… Created new VPC: ${VPC_ID}"
              fi
            else
              echo "âŒ No default VPC found and not creating new VPC."
              echo "Please provide a VPC ID or enable VPC creation for production."
              exit 1
            fi
          else
            echo "âœ… Using default VPC: ${VPC_ID}"
          fi
        fi
        
        # Store VPC ID
        echo "vpc_id=${VPC_ID}" >> $GITHUB_OUTPUT
        
        # Get subnets for this VPC
        SUBNETS=$(aws ec2 describe-subnets --filters "Name=vpc-id,Values=${VPC_ID}" --query 'Subnets[*].SubnetId' --output text)
        echo "Found subnets: ${SUBNETS}"
        
        # Store VPC ID in parameter store for future use
        aws ssm put-parameter \
          --name /${ENV_NAME}/vpc/id \
          --value ${VPC_ID} \
          --type String \
          --overwrite
    
    - name: Setup Database
      id: database
      if: github.event.inputs.action != 'destroy'
      run: |
        echo "ðŸ—„ï¸ Setting up RDS Database"
        echo "=========================="
        
        # Get VPC ID from previous step
        VPC_ID="${{ steps.vpc.outputs.vpc_id }}"
        
        # Check if DB already exists
        DB_EXISTS=$(aws rds describe-db-instances \
          --db-instance-identifier ${DB_IDENTIFIER} \
          --query 'DBInstances[0].DBInstanceIdentifier' \
          --output text 2>/dev/null || echo "NOT_FOUND")
        
        if [ "$DB_EXISTS" = "NOT_FOUND" ]; then
          echo "Creating new RDS instance..."
          
          # Generate secure password
          DB_PASSWORD=$(openssl rand -base64 32 | tr -d '/=+' | cut -c1-25)
          
          # Get subnets for DB subnet group
          SUBNETS=$(aws ec2 describe-subnets --filters "Name=vpc-id,Values=${VPC_ID}" --query 'Subnets[*].SubnetId' --output text)
          
          # Delete any existing DB subnet group to ensure clean setup
          echo "Setting up DB subnet group..."
          
          # Check if subnet group exists and if any DB is using it
          SUBNET_GROUP_EXISTS=$(aws rds describe-db-subnet-groups \
            --db-subnet-group-name ${DB_IDENTIFIER}-subnet-group \
            --query 'DBSubnetGroups[0].DBSubnetGroupName' \
            --output text 2>/dev/null || echo "NOT_FOUND")
          
          if [ "$SUBNET_GROUP_EXISTS" != "NOT_FOUND" ]; then
            echo "Found existing DB subnet group. Checking if it's in use..."
            
            # Check if any DB instances are using this subnet group
            DB_USING=$(aws rds describe-db-instances \
              --query "DBInstances[?DBSubnetGroup.DBSubnetGroupName=='${DB_IDENTIFIER}-subnet-group'].DBInstanceIdentifier" \
              --output text 2>/dev/null || echo "")
            
            if [ -z "$DB_USING" ]; then
              echo "No databases using the subnet group. Deleting it..."
              aws rds delete-db-subnet-group \
                --db-subnet-group-name ${DB_IDENTIFIER}-subnet-group 2>/dev/null || true
              sleep 2
            else
              echo "WARNING: Subnet group is being used by database: $DB_USING"
              echo "Will attempt to update it instead of recreating..."
            fi
          fi
          
          # Create new DB subnet group
          echo "Creating DB subnet group..."
          aws rds create-db-subnet-group \
            --db-subnet-group-name ${DB_IDENTIFIER}-subnet-group \
            --db-subnet-group-description "Subnet group for ${DB_IDENTIFIER}" \
            --subnet-ids ${SUBNETS} 2>/dev/null || {
            echo "Failed to create new subnet group. Attempting to modify existing one..."
            aws rds modify-db-subnet-group \
              --db-subnet-group-name ${DB_IDENTIFIER}-subnet-group \
              --db-subnet-group-description "Subnet group for ${DB_IDENTIFIER} (updated)" \
              --subnet-ids ${SUBNETS}
          }
          
          # Create security group for RDS
          SG_ID=$(aws ec2 create-security-group \
            --group-name ${DB_IDENTIFIER}-sg \
            --description "Security group for ${DB_IDENTIFIER}" \
            --vpc-id ${VPC_ID} \
            --query 'GroupId' \
            --output text 2>/dev/null || \
            aws ec2 describe-security-groups \
              --filters "Name=group-name,Values=${DB_IDENTIFIER}-sg" \
              --query 'SecurityGroups[0].GroupId' \
              --output text)
          
          # Allow PostgreSQL access
          aws ec2 authorize-security-group-ingress \
            --group-id ${SG_ID} \
            --protocol tcp \
            --port 5432 \
            --cidr 0.0.0.0/0 2>/dev/null || true
          
          # Create RDS instance
          aws rds create-db-instance \
            --db-instance-identifier ${DB_IDENTIFIER} \
            --db-instance-class db.t3.micro \
            --engine postgres \
            --master-username postgres \
            --master-user-password "${DB_PASSWORD}" \
            --allocated-storage 20 \
            --db-subnet-group-name ${DB_IDENTIFIER}-subnet-group \
            --vpc-security-group-ids ${SG_ID} \
            --publicly-accessible \
            --backup-retention-period 7 \
            --no-multi-az \
            --storage-encrypted
          
          echo "Waiting for database to be available..."
          aws rds wait db-instance-available --db-instance-identifier ${DB_IDENTIFIER}
          
          # Store password in Secrets Manager
          aws secretsmanager create-secret \
            --name ${DB_IDENTIFIER}-password \
            --secret-string "${DB_PASSWORD}" 2>/dev/null || \
            aws secretsmanager update-secret \
              --secret-id ${DB_IDENTIFIER}-password \
              --secret-string "${DB_PASSWORD}"
          
          echo "password=${DB_PASSWORD}" >> $GITHUB_OUTPUT
        else
          echo "Database already exists"
          # Retrieve password from Secrets Manager
          DB_PASSWORD=$(aws secretsmanager get-secret-value \
            --secret-id ${DB_IDENTIFIER}-password \
            --query 'SecretString' \
            --output text)
          echo "password=${DB_PASSWORD}" >> $GITHUB_OUTPUT
        fi
        
        # Get database endpoint
        DB_ENDPOINT=$(aws rds describe-db-instances \
          --db-instance-identifier ${DB_IDENTIFIER} \
          --query 'DBInstances[0].Endpoint.Address' \
          --output text)
        
        echo "endpoint=${DB_ENDPOINT}" >> $GITHUB_OUTPUT
        echo "Database endpoint: ${DB_ENDPOINT}"
    
    - name: Setup Application Load Balancer
      id: alb
      if: github.event.inputs.action != 'destroy'
      run: |
        echo "ðŸ”„ Setting up Application Load Balancer"
        echo "======================================"
        
        # Get VPC ID from previous step
        VPC_ID="${{ steps.vpc.outputs.vpc_id }}"
        
        # Get subnets (need at least 2 in different AZs for ALB)
        SUBNET_IDS=$(aws ec2 describe-subnets \
          --filters "Name=vpc-id,Values=${VPC_ID}" \
          --query 'Subnets[*].SubnetId' \
          --output text | tr '\t' ' ')
        
        # Check if ALB exists
        ALB_EXISTS=$(aws elbv2 describe-load-balancers \
          --names ${ALB_NAME} \
          --query 'LoadBalancers[0].LoadBalancerArn' \
          --output text 2>/dev/null || echo "NOT_FOUND")
        
        if [ "$ALB_EXISTS" = "NOT_FOUND" ]; then
          echo "Creating new ALB..."
          
          # Create security group for ALB
          ALB_SG_ID=$(aws ec2 create-security-group \
            --group-name ${ALB_NAME}-sg \
            --description "Security group for ${ALB_NAME}" \
            --vpc-id ${VPC_ID} \
            --query 'GroupId' \
            --output text)
          
          # Allow HTTP/HTTPS traffic
          aws ec2 authorize-security-group-ingress \
            --group-id ${ALB_SG_ID} \
            --protocol tcp \
            --port 80 \
            --cidr 0.0.0.0/0
          
          aws ec2 authorize-security-group-ingress \
            --group-id ${ALB_SG_ID} \
            --protocol tcp \
            --port 443 \
            --cidr 0.0.0.0/0 2>/dev/null || true
          
          # Create ALB
          ALB_ARN=$(aws elbv2 create-load-balancer \
            --name ${ALB_NAME} \
            --subnets ${SUBNET_IDS} \
            --security-groups ${ALB_SG_ID} \
            --scheme internet-facing \
            --type application \
            --ip-address-type ipv4 \
            --query 'LoadBalancers[0].LoadBalancerArn' \
            --output text)
          
          # Create target groups
          # Backend target group - EXTREMELY LENIENT health checks (services keep running)
          BACKEND_TG_ARN=$(aws elbv2 create-target-group \
            --name ${ENV_NAME}-backend-tg-80 \
            --protocol HTTP \
            --port 8082 \
            --vpc-id ${VPC_ID} \
            --target-type ip \
            --health-check-path / \
            --health-check-interval-seconds 300 \
            --health-check-timeout-seconds 120 \
            --healthy-threshold-count 2 \
            --unhealthy-threshold-count 10 \
            --query 'TargetGroups[0].TargetGroupArn' \
            --output text)
          
          # Frontend target group
          FRONTEND_TG_ARN=$(aws elbv2 create-target-group \
            --name ${ENV_NAME}-frontend-tg-80 \
            --protocol HTTP \
            --port 80 \
            --vpc-id ${VPC_ID} \
            --target-type ip \
            --health-check-path / \
            --query 'TargetGroups[0].TargetGroupArn' \
            --output text)
          
          # Create HTTP listener with rules
          LISTENER_ARN=$(aws elbv2 create-listener \
            --load-balancer-arn ${ALB_ARN} \
            --protocol HTTP \
            --port 80 \
            --default-actions Type=forward,TargetGroupArn=${FRONTEND_TG_ARN} \
            --query 'Listeners[0].ListenerArn' \
            --output text)
          
          # Add rule for backend /api/*
          aws elbv2 create-rule \
            --listener-arn ${LISTENER_ARN} \
            --priority 10 \
            --conditions Field=path-pattern,Values="/api/*" \
            --actions Type=forward,TargetGroupArn=${BACKEND_TG_ARN}
          
          # Store target group ARNs in parameter store
          aws ssm put-parameter \
            --name /${ENV_NAME}/backend/target-group-arn \
            --value ${BACKEND_TG_ARN} \
            --type String \
            --overwrite
          
          aws ssm put-parameter \
            --name /${ENV_NAME}/frontend/target-group-arn \
            --value ${FRONTEND_TG_ARN} \
            --type String \
            --overwrite
        fi
        
        # Get ALB DNS name
        ALB_DNS=$(aws elbv2 describe-load-balancers \
          --names ${ALB_NAME} \
          --query 'LoadBalancers[0].DNSName' \
          --output text)
        
        echo "dns_name=${ALB_DNS}" >> $GITHUB_OUTPUT
        echo "ALB DNS: ${ALB_DNS}"
    
    - name: Setup ECS Cluster
      if: github.event.inputs.action != 'destroy'
      run: |
        echo "ðŸš¢ Setting up ECS Cluster"
        echo "========================"
        
        CLUSTER_NAME="${{ env.PROJECT_NAME }}-cluster"
        
        # Check if cluster exists
        CLUSTER_EXISTS=$(aws ecs describe-clusters \
          --clusters ${CLUSTER_NAME} \
          --query 'clusters[?status==`ACTIVE`].clusterName' \
          --output text 2>/dev/null || echo "NOT_FOUND")
        
        if [ "$CLUSTER_EXISTS" = "NOT_FOUND" ] || [ -z "$CLUSTER_EXISTS" ]; then
          echo "Creating new ECS cluster..."
          aws ecs create-cluster \
            --cluster-name ${CLUSTER_NAME} \
            --capacity-providers FARGATE FARGATE_SPOT \
            --default-capacity-provider-strategy capacityProvider=FARGATE,weight=1
          echo "âœ… ECS cluster created: ${CLUSTER_NAME}"
        else
          echo "âœ… ECS cluster already exists: ${CLUSTER_NAME}"
        fi
        
        # Create ECR repositories if they don't exist
        echo "Setting up ECR repositories..."
        for REPO in backend frontend; do
          aws ecr describe-repositories \
            --repository-names ${PROJECT_NAME}-${REPO} 2>/dev/null || \
          aws ecr create-repository \
            --repository-name ${PROJECT_NAME}-${REPO} \
            --image-scanning-configuration scanOnPush=true
          echo "âœ… ECR repository ready: ${PROJECT_NAME}-${REPO}"
        done
    
    - name: Store Infrastructure Details
      if: github.event.inputs.action != 'destroy'
      run: |
        echo "ðŸ’¾ Storing Infrastructure Details"
        echo "================================"
        
        # Store all infrastructure details in Parameter Store
        aws ssm put-parameter \
          --name /${ENV_NAME}/database/endpoint \
          --value "${{ steps.database.outputs.endpoint }}" \
          --type String \
          --overwrite
        
        aws ssm put-parameter \
          --name /${ENV_NAME}/alb/dns \
          --value "${{ steps.alb.outputs.dns_name }}" \
          --type String \
          --overwrite
        
        echo "âœ… Infrastructure setup complete!"
        echo ""
        echo "ðŸŒ Access URLs:"
        echo "============="
        echo "Frontend: http://${{ steps.alb.outputs.dns_name }}"
        echo "Backend API: http://${{ steps.alb.outputs.dns_name }}/api"
        echo "Backend Swagger: http://${{ steps.alb.outputs.dns_name }}/api/swagger-ui/index.html"
        echo ""
        echo "ðŸ“ Infrastructure Details:"
        echo "======================="
        echo "VPC ID: ${{ steps.vpc.outputs.vpc_id }}"
        echo "Database Endpoint: ${{ steps.database.outputs.endpoint }}"
        echo "ALB DNS: ${{ steps.alb.outputs.dns_name }}"
        echo ""
        echo "ðŸ”‘ Database Credentials:"
        echo "======================"
        echo "Username: postgres"
        echo "Password: Stored in AWS Secrets Manager (${DB_IDENTIFIER}-password)"
        echo "Database Name: sabpaisa_tokenization"
    
    - name: Cleanup Infrastructure
      if: github.event.inputs.action == 'destroy'
      run: |
        echo "ðŸ—‘ï¸ Destroying Infrastructure"
        echo "=========================="
        
        # Delete RDS instance
        DB_EXISTS=$(aws rds describe-db-instances \
          --db-instance-identifier ${DB_IDENTIFIER} \
          --query 'DBInstances[0].DBInstanceStatus' \
          --output text 2>/dev/null || echo "NOT_FOUND")
        
        if [ "$DB_EXISTS" != "NOT_FOUND" ]; then
          echo "Deleting RDS instance..."
          aws rds delete-db-instance \
            --db-instance-identifier ${DB_IDENTIFIER} \
            --skip-final-snapshot \
            --delete-automated-backups
          
          echo "Waiting for database deletion..."
          aws rds wait db-instance-deleted \
            --db-instance-identifier ${DB_IDENTIFIER} || true
        fi
        
        # Delete DB subnet group (only after DB is deleted)
        echo "Deleting DB subnet group..."
        aws rds delete-db-subnet-group \
          --db-subnet-group-name ${DB_IDENTIFIER}-subnet-group 2>/dev/null || true
        
        # Delete ALB
        ALB_ARN=$(aws elbv2 describe-load-balancers \
          --names ${ALB_NAME} \
          --query 'LoadBalancers[0].LoadBalancerArn' \
          --output text 2>/dev/null || echo "NOT_FOUND")
        
        if [ "$ALB_ARN" != "NOT_FOUND" ]; then
          # Delete listeners first
          LISTENER_ARNS=$(aws elbv2 describe-listeners \
            --load-balancer-arn ${ALB_ARN} \
            --query 'Listeners[*].ListenerArn' \
            --output text)
          
          for LISTENER_ARN in ${LISTENER_ARNS}; do
            aws elbv2 delete-listener --listener-arn ${LISTENER_ARN} 2>/dev/null || true
          done
          
          # Delete target groups
          for TG_NAME in ${ENV_NAME}-backend-tg-80 ${ENV_NAME}-frontend-tg-80; do
            TG_ARN=$(aws elbv2 describe-target-groups \
              --names ${TG_NAME} \
              --query 'TargetGroups[0].TargetGroupArn' \
              --output text 2>/dev/null || echo "NOT_FOUND")
            
            if [ "$TG_ARN" != "NOT_FOUND" ]; then
              aws elbv2 delete-target-group --target-group-arn ${TG_ARN} 2>/dev/null || true
            fi
          done
          
          # Delete load balancer
          aws elbv2 delete-load-balancer --load-balancer-arn ${ALB_ARN}
        fi
        
        # Delete ECS services if they exist
        CLUSTER_NAME="${{ env.PROJECT_NAME }}-cluster"
        for SERVICE in backend-service-${ENV_NAME} frontend-service-${ENV_NAME}; do
          aws ecs update-service \
            --cluster ${CLUSTER_NAME} \
            --service ${SERVICE} \
            --desired-count 0 2>/dev/null || true
          
          aws ecs delete-service \
            --cluster ${CLUSTER_NAME} \
            --service ${SERVICE} \
            --force 2>/dev/null || true
        done
        
        # Note: We're not deleting the ECS cluster or VPC as they might be shared
        
        # Delete security groups
        for SG_NAME in ${DB_IDENTIFIER}-sg ${ALB_NAME}-sg; do
          SG_ID=$(aws ec2 describe-security-groups \
            --filters "Name=group-name,Values=${SG_NAME}" \
            --query 'SecurityGroups[0].GroupId' \
            --output text 2>/dev/null || echo "NOT_FOUND")
          
          if [ "$SG_ID" != "NOT_FOUND" ]; then
            aws ec2 delete-security-group --group-id ${SG_ID} 2>/dev/null || true
          fi
        done
        
        # Delete parameters
        aws ssm delete-parameter --name /${ENV_NAME}/database/endpoint 2>/dev/null || true
        aws ssm delete-parameter --name /${ENV_NAME}/backend/target-group-arn 2>/dev/null || true
        aws ssm delete-parameter --name /${ENV_NAME}/frontend/target-group-arn 2>/dev/null || true
        aws ssm delete-parameter --name /${ENV_NAME}/alb/dns 2>/dev/null || true
        aws ssm delete-parameter --name /${ENV_NAME}/vpc/id 2>/dev/null || true
        
        # Delete secrets
        aws secretsmanager delete-secret \
          --secret-id ${DB_IDENTIFIER}-password \
          --force-delete-without-recovery 2>/dev/null || true
        
        echo "âœ… Cleanup complete"
        echo ""
        echo "Note: VPC and ECS Cluster were not deleted as they might be shared resources."
        echo "If you want to delete them, please do so manually."